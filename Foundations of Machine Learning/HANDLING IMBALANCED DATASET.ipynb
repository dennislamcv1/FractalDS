{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa924ab",
   "metadata": {
    "id": "4aa924ab"
   },
   "source": [
    "### VIDEO 9: HANDLING IMBALANCED DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425bc34",
   "metadata": {
    "id": "6425bc34"
   },
   "source": [
    "Hello learners!\n",
    "\n",
    "In this video, let's use the techniques that we have learnt in the last video to handle class imbalance. Apart from that, we will also be using oversampling and undersampling techniques to balance the dataset. So, let's get started!\n",
    "\n",
    "First let's do preprocessing in the data just like we are doing while building the classification and regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db7bcba",
   "metadata": {
    "id": "4db7bcba"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Loading the data\n",
    "data = pd.read_csv(\"Synergix_data_preprocessed_new.csv\")\n",
    "\n",
    "#Storing the ratio in a list named Rating_ratio\n",
    "Rating_ratio = []\n",
    "for row in data.values:\n",
    "    if(row[4]+row[5] == 0):\n",
    "        if(row[7]+row[8] == 0):\n",
    "            #If all the ratings are zero then overall rating ratio will also be zero\n",
    "            Rating_ratio.append(0.0)\n",
    "        else:\n",
    "            #If only the numerator(1 and 2 star) ratings are zero then adding -99999 to the list temporarily which\n",
    "            #will be taken care of in the next cell.\n",
    "            Rating_ratio.append(-99999)\n",
    "    else:\n",
    "        Rating_ratio.append((int(row[7])+(row[8]))/(int(row[4])+int(row[5])))\n",
    "\n",
    "#replacing -99999 with the maximum ratio in the list\n",
    "max_rating = max(Rating_ratio)\n",
    "for x in range(len(Rating_ratio)):\n",
    "    if(Rating_ratio[x] == -99999):\n",
    "        Rating_ratio[x] = max_rating\n",
    "\n",
    "#adding the column 'Good_By_Bad_Rating' to the dataframe\n",
    "data['Good_By_Bad_Rating'] = Rating_ratio\n",
    "\n",
    "data = data.drop(columns = ['1_Star_Rating', '2_Star_Rating', '3_Star_Rating', '4_Star_Rating', '5_Star_Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4071f6",
   "metadata": {
    "id": "fe4071f6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "data [['Segment']]= data [['Segment']].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c7552b",
   "metadata": {
    "id": "25c7552b"
   },
   "outputs": [],
   "source": [
    "data = data.drop(columns = 'Units_sold',axis=1)\n",
    "\n",
    "X = data.drop(columns = 'Units_sold>1000')\n",
    "y = data['Units_sold>1000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a265f0bf",
   "metadata": {
    "id": "a265f0bf"
   },
   "outputs": [],
   "source": [
    "# Importing the train-test split from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Performing train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4dc747",
   "metadata": {
    "id": "2d4dc747"
   },
   "source": [
    "Before we start the stratified split, let's check the ratio of both the classes in the y_train and y_test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e762bf7a",
   "metadata": {
    "id": "e762bf7a",
    "outputId": "5642d6ff-937c-47a6-d6f2-220f8ac382bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Units_sold>1000\n",
       "1    0.598137\n",
       "0    0.401863\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60650a6a",
   "metadata": {
    "id": "60650a6a",
    "outputId": "9cf6f734-8b21-431c-d224-9904b56ee1a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Units_sold>1000\n",
       "1    0.614261\n",
       "0    0.385739\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6e1b4",
   "metadata": {
    "id": "b9d6e1b4"
   },
   "source": [
    "To incorporate startify split in out model, we can use train_test_split with the stratify parameter set to ‘y’ to ensure that the split is stratified based on the target variable y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191f80a",
   "metadata": {
    "id": "d191f80a"
   },
   "source": [
    "Now, let's perform the stratified split and see if this ratio changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b337fb0f",
   "metadata": {
    "id": "b337fb0f"
   },
   "outputs": [],
   "source": [
    "# Performing train test split with stratification\n",
    "X_train_st, X_test_st, y_train_st, y_test_st = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f552e8a8",
   "metadata": {
    "id": "f552e8a8",
    "outputId": "69739b1a-78a0-4efe-f467-94a98f0435a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Units_sold>1000\n",
       "1    0.60294\n",
       "0    0.39706\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_st.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4f6791",
   "metadata": {
    "id": "ef4f6791",
    "outputId": "bcf9c6f2-535f-4aef-f3c7-eeeec1d55374"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Units_sold>1000\n",
       "1    0.603056\n",
       "0    0.396944\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_st.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517bc58",
   "metadata": {
    "id": "7517bc58"
   },
   "source": [
    "As you can see, the ratio does not change much, this is because, the dataset that we have is large, therefore the chances of sample bias are small.  But as a good practice, for classification problems with imbalanced dataset, we should always do stratified split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f886586",
   "metadata": {
    "id": "1f886586"
   },
   "source": [
    "Now let's try the other technique which is using the hyperparameter: class weight.\n",
    "As discussed earlier specific machine learning algorithms like Decision Tree, Logistic Regression etc allows us to set the class weight hyperparameter. Let's set it to ‘balanced’ and use the stratified splits we have obtained earlier to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a036a51",
   "metadata": {
    "id": "2a036a51"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT_model = DecisionTreeClassifier(max_depth = 11, min_samples_leaf= 6, random_state=42, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee96f2",
   "metadata": {
    "id": "27ee96f2"
   },
   "source": [
    "However, we can also assign customised class weights by defining a dictionary where the keys represent the class labels, and #the values represent the weights we want to assign to each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3dfbb5",
   "metadata": {
    "id": "bc3dfbb5"
   },
   "source": [
    "DT_model = DecisionTreeClassifier(max_depth = 11, min_samples_leaf= 6, random_state=42, class_weight = {0:0.8, 1:0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d704f459",
   "metadata": {
    "id": "d704f459",
    "outputId": "e97f0b8d-2159-470f-d170-596bbba2a94a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;, max_depth=11,\n",
       "                       min_samples_leaf=6, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;, max_depth=11,\n",
       "                       min_samples_leaf=6, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', max_depth=11,\n",
       "                       min_samples_leaf=6, random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "DT_model.fit(X_train_st, y_train_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb0943",
   "metadata": {
    "id": "5acb0943"
   },
   "source": [
    "Now, let's make the predictions on the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a742c4e",
   "metadata": {
    "id": "6a742c4e",
    "outputId": "d7c36aea-f59b-4762-a22d-15bf29826ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.8870789957134109\n",
      "Test score:  0.8261986301369862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Make predictions on the train dataset\n",
    "y_train_pred = DT_model.predict(X_train_st)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_test_pred = DT_model.predict(X_test_st)\n",
    "\n",
    "# Let's display the model performance on the train and test data.\n",
    "\n",
    "print('Train score: ', f1_score(y_train_st, y_train_pred))\n",
    "print('Test score: ', f1_score(y_test_st, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfafbb",
   "metadata": {
    "id": "0fcfafbb"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4adaab1",
   "metadata": {
    "id": "e4adaab1"
   },
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d18d0",
   "metadata": {
    "id": "3e0d18d0"
   },
   "source": [
    "Now that we have implemented the tricks to handle class imbalance, let's dive deeper to understand how to balance the dataset using undersampling and oversampling techniques. First, let's do undersampling!\n",
    "\n",
    "To do so, we have to first install a ski-kit learn library called 'imblearn'. It is specifically designed to deal with imbalanced datasets and help us seamlessly implement various methods like undersampling, oversampling, and SMOTE etc. Write the following code to install the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6878c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93921b1d",
   "metadata": {
    "id": "d624ec6b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f71c50",
   "metadata": {
    "id": "c5f71c50"
   },
   "source": [
    "Once the library is installed, let's import RandomUnderSampler from imblearn.under_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6381ea38",
   "metadata": {
    "id": "6381ea38"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca16934",
   "metadata": {
    "id": "8ca16934"
   },
   "source": [
    "Now, let's resample the training data and build the decision tree with the resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa7d5348",
   "metadata": {
    "id": "fa7d5348"
   },
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state = 42)\n",
    "X_train_rus, y_train_rus = sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e0d0ac",
   "metadata": {
    "id": "31e0d0ac",
    "outputId": "0e1591b4-1f6a-4dd8-bdd3-cce61eb7a12c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Units_sold>1000\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train_rus.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2488f2",
   "metadata": {
    "id": "6f2488f2"
   },
   "source": [
    "As you can see from the ratios, the class distribution is equal now due to random undersampling.\n",
    "Let's build the model using these modified training data and see its performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88cfd96",
   "metadata": {
    "id": "d88cfd96",
    "outputId": "dddcf0fc-76b1-4082-850e-c72977bfea06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score:  0.8667085539897674\n",
      "Test F1 Score:  0.8229858504187121\n"
     ]
    }
   ],
   "source": [
    "DT_model = DecisionTreeClassifier(max_depth = 11, min_samples_leaf= 6, random_state=42)\n",
    "\n",
    "DT_model.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "y_train_pred = DT_model.predict(X_train_rus)\n",
    "y_pred = DT_model.predict(X_test)\n",
    "\n",
    "\n",
    "print('Train F1 Score: ', f1_score(y_train_rus, y_train_pred))\n",
    "print('Test F1 Score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf6d86",
   "metadata": {
    "id": "a1cf6d86"
   },
   "source": [
    "As you can see, the model performance has deteriorated slightly. This is because, in undersampling instances from the majority class are  randomly removed and this may lead to loss of information. Let's try random oversampling now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e84067",
   "metadata": {
    "id": "57e84067"
   },
   "source": [
    "### Random oversampling\n",
    "\n",
    "Performing random oversampling involves doing the same steps like we did for random undersampler. However, the only difference is we have to import RandomOverSampler from imblearn.over_sampling and build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "265e65e6",
   "metadata": {
    "id": "265e65e6"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b27e2",
   "metadata": {
    "id": "066b27e2"
   },
   "source": [
    "Let's perform resampling of the orginial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41fce8",
   "metadata": {
    "id": "0b41fce8"
   },
   "outputs": [],
   "source": [
    "sampler = RandomOverSampler(random_state = 42)\n",
    "X_train_ros, y_train_ros = sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfc42e",
   "metadata": {
    "id": "9acfc42e"
   },
   "source": [
    "Once random oversampling is done we can quickly check if the ratio of the classes are equal now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdfe36",
   "metadata": {
    "id": "39cdfe36",
    "outputId": "029ee433-4c62-4dc2-c62d-a225ac10cd8a"
   },
   "outputs": [],
   "source": [
    "y_train_ros.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2e8d6",
   "metadata": {
    "id": "15e2e8d6"
   },
   "source": [
    "As you can see, it's equal. Let's go to the next stepn and build the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae8273",
   "metadata": {
    "id": "61ae8273",
    "outputId": "c2a6b1b9-27dd-4e43-8f9f-b52896329be2"
   },
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier(max_depth = 11, min_samples_leaf= 6, random_state=42)\n",
    "\n",
    "DT_model.fit(X_train_ros, y_train_ros)\n",
    "\n",
    "y_train_pred = DT_model.predict(X_train_ros)\n",
    "y_pred = DT_model.predict(X_test)\n",
    "\n",
    "y_train_pred = DT_model.predict(X_train_ros)\n",
    "y_pred = DT_model.predict(X_test)\n",
    "\n",
    "# Printing the F1 score of the train and test data\n",
    "print('Train F1 Score: ', f1_score(y_train_ros, y_train_pred))\n",
    "print('Test F1 Score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bfc9fd",
   "metadata": {
    "id": "d4bfc9fd"
   },
   "source": [
    "As you can see, this performance is slightly better compared to the undersampling scenario as no information has been removed from the dataset. Let's try impleting anotherv oversampling technique which is SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d9b38",
   "metadata": {
    "id": "1d5d9b38"
   },
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc14099",
   "metadata": {
    "id": "ebc14099"
   },
   "source": [
    "let's starts by importing the SMOTE (Synthetic Minority Over-sampling Technique) method from the imblearn.over_sampling module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff675a",
   "metadata": {
    "id": "06ff675a"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce1a99",
   "metadata": {
    "id": "4fce1a99"
   },
   "source": [
    "Next, SMOTE is initialized with a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8867ba",
   "metadata": {
    "id": "9f8867ba"
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708b4bf",
   "metadata": {
    "id": "0708b4bf"
   },
   "source": [
    "SMOTE is applied to the training data resulting in oversampled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a0e81",
   "metadata": {
    "id": "132a0e81"
   },
   "outputs": [],
   "source": [
    "X_train_smt, y_train_smt = smote.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675e29aa",
   "metadata": {
    "id": "675e29aa"
   },
   "source": [
    "Let's quickly check the ratio of the classes in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94eb7c",
   "metadata": {
    "id": "7f94eb7c",
    "outputId": "caae9fa7-fa2e-4752-f7e7-9c5ec8cfc4a6"
   },
   "outputs": [],
   "source": [
    "y_train_smt.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf804b",
   "metadata": {
    "id": "eacf804b"
   },
   "source": [
    "Let's build the decision tree model using the oversampled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73281a",
   "metadata": {
    "id": "dd73281a",
    "outputId": "cfb87bac-f003-40e7-8577-404f9e5d6470"
   },
   "outputs": [],
   "source": [
    "DT_model = DecisionTreeClassifier(max_depth = 11, min_samples_leaf= 6, random_state=42)\n",
    "DT_model.fit(X_train_smt, y_train_smt)\n",
    "\n",
    "\n",
    "#Making predictions\n",
    "y_train_pred = DT_model.predict(X_train_smt)\n",
    "y_pred = DT_model.predict(X_test)\n",
    "\n",
    "#Evaluating the model\n",
    "print('Training F1 score: ', f1_score(y_train_smt, y_train_pred))\n",
    "print('Testing F1 score: ', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a70cde",
   "metadata": {
    "id": "f9a70cde"
   },
   "source": [
    "As you can see, in this scenario, with SMOTE, the model performance is similar to random oversampling. As we know, iterating different techniques is an essential part of machine learning model building process. So  it is advisable to try out different techniques and tools to get the best performance.  In the next video, we will compare the performance of all the models that we have built so far. So see you in the next one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5021b9",
   "metadata": {
    "id": "eb5021b9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
